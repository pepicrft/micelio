# Micelio + hif: The Complete Vision

**The future of software development is agent-first. We're building the infrastructure to support it.**

---

## The Big Picture

Micelio is not just another Git forge. It's a complete reimagining of version control and software collaboration for an AI-native world.

**Two interconnected projects:**

1. **hif** - Revolutionary version control system designed for agent-first workflows
2. **Micelio** - Modern forge built specifically for hif (like GitHub is to Git)

Together, they solve the fundamental problem: **Git tracks what happened. We need systems that track why.**

---

## The Problem We're Solving

### Current Reality (Git + GitHub)
- **Snapshot-based** - commits are frozen pictures, iterations invisible
- **Human-centric** - designed for think-then-commit workflows  
- **Linear scaling** - performance degrades with repo size and activity
- **Branch complexity** - merges, rebases, conflicts become unwieldy at scale
- **Lost context** - reasoning, alternatives, conversations happen outside VCS

### Agent-First Future
- **Hundreds of AI agents** working concurrently on codebases
- **Billions of files** in monorepos (Meta/Google scale)
- **Hundreds of thousands of changes per day**
- **Continuous reasoning** - agents explore, backtrack, iterate, decide
- **Human oversight** - reviewing and directing rather than writing most code

**Git can't handle this future. We need something fundamentally different.**

---

## Our Solution

### hif: Version Control Reimagined

**Philosophy:** "Git tracks what. hif tracks why."

#### Core Innovation: Sessions (Not Commits)
Every unit of work is a **session** containing:
- ðŸŽ¯ **Goal** - what you're trying to accomplish
- ðŸ’¬ **Conversation** - discussion between agents and humans  
- ðŸ§  **Decisions** - why things were done a certain way
- ðŸ“ **Changes** - the actual file modifications

```
Session: "Add authentication to API"
â”œâ”€â”€ Goal: Implement secure login/logout endpoints
â”œâ”€â”€ Conversation
â”‚   â”œâ”€â”€ Human: "Use JWT tokens for auth"
â”‚   â”œâ”€â”€ Agent: "Should I store sessions in Redis?"
â”‚   â”œâ”€â”€ Human: "No, keep JWT stateless"
â”‚   â””â”€â”€ Agent: "Implementing with bcrypt for passwords"
â”œâ”€â”€ Decisions
â”‚   â”œâ”€â”€ "JWT chosen over sessions per human preference"
â”‚   â”œâ”€â”€ "Bcrypt for password hashing - industry standard"
â”‚   â””â”€â”€ "Auth middleware in /middleware - follows existing pattern"
â””â”€â”€ Changes
    â”œâ”€â”€ + src/auth/jwt.zig
    â”œâ”€â”€ + src/middleware/auth.zig
    â””â”€â”€ ~ src/main.zig (added auth routes)
```

#### Technical Architecture
- **Forge-first** - server is source of truth, not local disk
- **Object storage-first** - S3 as primary storage (like Turbopuffer)
- **Stateless agents** - no coordinator bottleneck (like WarpStream)
- **O(log n) operations** - bloom filters for conflict detection
- **Binary everywhere** - no JSON, optimized for performance
- **Coordinator-free landing** - S3 conditional writes for atomicity

### Micelio: The Forge for hif

**Built with Elixir/Phoenix** - A modern, minimalist forge designed specifically for hif workflows.

#### Key Features
- **Session-based workflows** - browse reasoning, not just code changes
- **Agent collaboration tools** - built for human + AI teams
- **Minimal UI** - focus on essential workflows, not feature bloat
- **Self-hostable** - your code, your infrastructure, your control
- **Open source** - GPL-2.0, following Git's lineage

#### Architecture Highlights
- **Stateless web agents** - any server handles any request
- **hif integration via Zig NIFs** - native performance through C FFI
- **SQLite for auth only** - users, tokens, permissions (~KB per user)
- **S3 for everything else** - repositories, sessions, file trees

---

## Why This Matters

### For Individual Developers
- **Capture reasoning** - never lose context of why decisions were made
- **Agent collaboration** - seamless handoffs between human and AI work
- **True history** - see the actual development process, not just snapshots
- **Reduced cognitive load** - systems that remember so you don't have to

### For Teams
- **Transparent decision-making** - everyone sees the why, not just the what
- **Efficient code review** - review reasoning and decisions, not just diffs
- **Knowledge preservation** - team knowledge captured in version control
- **Agent integration** - AI agents as first-class team members

### For Organizations
- **Scale beyond Git limitations** - handle massive monorepos efficiently
- **Audit trail** - complete reasoning chain for compliance/security
- **Faster onboarding** - new team members see historical decision context
- **Future-proof** - ready for the agent-first development paradigm

---

## Project Status & Roadmap

### Current State (January 2026)
- âš ï¸ **Work in progress** - not ready for production use
- âœ… **hif core** - Zig implementation with C FFI
- âœ… **Micelio forge** - Elixir/Phoenix web application  
- âœ… **Basic workflows** - session start/land operations
- ðŸš§ **Active development** - rapid iteration on core concepts

### Next Milestones
1. **Session UI** - browse sessions with conversation/decision history
2. **Conflict resolution** - merge sessions with overlapping changes
3. **Performance optimization** - handle large repositories efficiently
4. **Agent SDK** - libraries for AI agents to use hif directly
5. **Migration tools** - import from Git repositories

### Long-term Vision
- **Industry adoption** - become the standard for agent-first development
- **Ecosystem growth** - tools, integrations, hosted solutions
- **Forge network** - federated instances like Git hosting today
- **AI-native workflows** - new paradigms we can't imagine yet

---

## Technical Deep Dive

For detailed technical architecture, algorithms, and implementation details, see:
- [`hif/DESIGN.md`](./hif/DESIGN.md) - Complete technical specification
- [`hif/PLAN.md`](./hif/PLAN.md) - Implementation roadmap
- [`hif/README.md`](./hif/README.md) - Quick start guide

---

## Contributing

We're in early development but welcome:
- **Feedback** on core concepts and user experience
- **Code contributions** to hif core and Micelio forge
- **Documentation** improvements and examples
- **Testing** with real repositories and workflows

See individual project READMEs for development setup.

---

## Philosophy

We believe the future of software development is collaborative intelligence - humans and AI agents working together as peers. This requires new tools designed from the ground up for this reality.

Git was revolutionary for its time, enabling distributed human collaboration at unprecedented scale. But the world has changed. We need systems that capture not just what we built, but how we reasoned, why we chose alternatives, and how we can learn from the process.

**hif + Micelio is our bet on that future.**

---

## Agent-First Build System Architecture [TO REVIEW/VALIDATE]

### The Nix + S3 Integration Model

**Core insight:** Agents need local validation they can trust, but the forge needs stateless, scalable execution and caching.

#### Nix's Role: Environment Reproducibility
- **flake.nix defines everything:** dependencies, build steps, test environments
- **Local agent validation:** `nix develop --command make test` gives instant feedback
- **Reproducible anywhere:** same Nix derivation = identical environment (agent machine = remote = prod)
- **Content addressing:** Nix's `/nix/store/hash-package` model aligns with S3 content-addressable storage

#### S3's Role: Stateless Persistence & Distribution
```
S3 Bucket Structure:
â”œâ”€â”€ derivations/
â”‚   â””â”€â”€ sha256:abc123.drv â†’ Nix derivation definitions
â”œâ”€â”€ artifacts/  
â”‚   â””â”€â”€ sha256:def456/ â†’ build outputs, binaries, assets
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ builds/sha256:ghi789 â†’ complete build results
â”‚   â”œâ”€â”€ tests/sha256:jkl012 â†’ test execution results  
â”‚   â””â”€â”€ telemetry/sha256:mno345 â†’ timing, resource usage
â”œâ”€â”€ execution-logs/
â”‚   â””â”€â”€ sha256:pqr678 â†’ full build/test output logs
â””â”€â”€ attestations/
    â””â”€â”€ sha256:stu901 â†’ cryptographic proof of execution
```

#### Agent Build Workflow
```
1. Agent modifies code in hif session
2. Build system generates Nix derivation from changes
3. Check S3 for existing artifact: GET /artifacts/sha256:computed-hash
4. Cache miss â†’ Execute locally: nix-build derivation  
5. Cache hit â†’ Skip build, validate locally: nix develop --command make verify
6. Upload results to S3: PUT /artifacts/sha256:new-hash
7. All tests pass â†’ hif land (session includes build attestation)
```

#### Remote Execution Integration
```
For heavy builds or special capabilities:
â”œâ”€â”€ Agent generates Nix derivation locally
â”œâ”€â”€ Submits to remote execution queue (stored in S3)
â”œâ”€â”€ Remote workers:
â”‚   â”œâ”€â”€ Fetch derivation from S3
â”‚   â”œâ”€â”€ Execute in identical Nix environment  
â”‚   â”œâ”€â”€ Upload artifacts back to S3
â”‚   â””â”€â”€ Signal completion via S3 event
â””â”€â”€ Agent gets notification, validates results locally
```

#### Security & Secrets Model
```
Capability-based access via S3 policies:
â”œâ”€â”€ Agent identity: arn:aws:iam::account:role/agent-session-abc123
â”œâ”€â”€ Scoped permissions:
â”‚   â”œâ”€â”€ s3:GetObject on artifacts/* (read builds)
â”‚   â”œâ”€â”€ s3:PutObject on artifacts/session-abc123/* (write own builds)
â”‚   â””â”€â”€ secretsmanager:GetSecretValue for session-scoped secrets
â”œâ”€â”€ Time-bound: role expires with hif session
â””â”€â”€ Audit trail: CloudTrail logs every S3/secrets access
```

#### Build Cache Optimization
```
Content-addressable caching strategy:
â”œâ”€â”€ Input hash: source + dependencies + build script + Nix derivation
â”œâ”€â”€ S3 check: artifacts/sha256:input-hash exists?
â”œâ”€â”€ Cache hit: Download artifact, verify locally with Nix
â”œâ”€â”€ Cache miss: Build locally/remotely, upload to S3
â””â”€â”€ Global sharing: all agents benefit from each other's builds
```

#### Stateless Forge Workers
```
Micelio forge workers (Elixir/Phoenix):
â”œâ”€â”€ No local state: everything in S3
â”œâ”€â”€ Build requests: generate Nix derivations, queue in S3
â”œâ”€â”€ Status queries: check S3 for completion
â”œâ”€â”€ Artifact serving: presigned S3 URLs for downloads
â””â”€â”€ Auto-scaling: workers are completely stateless
```

#### Integration with hif Sessions
```
Session: "Add payment gateway integration"
â”œâ”€â”€ Goal: Integrate Stripe API safely
â”œâ”€â”€ Build Context:
â”‚   â”œâ”€â”€ Nix derivation: payment-gateway.nix (reproducible env)
â”‚   â”œâ”€â”€ S3 artifacts: sha256:abc123 (cached build outputs)
â”‚   â”œâ”€â”€ Test results: sha256:def456 (integration test pass)
â”‚   â””â”€â”€ Security attestation: sha256:ghi789 (secrets access logged)
â”œâ”€â”€ Decisions: 
â”‚   â”œâ”€â”€ "Used Stripe test keys for integration tests"
â”‚   â””â”€â”€ "All tests pass in identical production environment"
â””â”€â”€ Land: Session includes cryptographic proof builds work
```

#### Why This Architecture Works

**For Agents:**
- Instant local feedback via Nix
- Confidence: local success = production success  
- Autonomous: no waiting for CI queues
- Secure: capability-based secret access

**For Organizations:**
- Scalable: S3 handles petabytes, millions of artifacts
- Cost-effective: pay only for storage used, workers auto-scale
- Auditable: every build, test, secret access logged
- Reproducible: bit-for-bit identical builds anywhere

**For the Forge:**
- Stateless: workers can restart/scale without losing state
- Global: S3 provides worldwide CDN for build artifacts
- Reliable: 11 nines durability, no backup needed
- Simple: no complex distributed caching layer

This model gives agents the speed of local development with the confidence of enterprise-grade CI/CD, while keeping the forge architecture stateless and scalable.

---

## hif Build Cache Daemon [TO REVIEW/VALIDATE]

### Architecture: Local Daemon + Protocol Translation

**Inspired by [Fabrik's](https://github.com/tuist/fabrik) proven architecture**, hif implements a local daemon that speaks existing build system protocols while providing S3-backed global caching.

#### Core Design Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    hif daemon                           â”‚
â”‚                  (per-session)                          â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Bazel Protocol  â”‚ â”‚ Gradle Protocol â”‚ â”‚Docker Reg.  â”‚ â”‚
â”‚ â”‚ (gRPC Remote    â”‚ â”‚ (HTTP Build     â”‚ â”‚(Layer Cache)â”‚ â”‚
â”‚ â”‚  Cache API)     â”‚ â”‚  Cache API)     â”‚ â”‚             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚            hif Session Engine                       â”‚ â”‚
â”‚ â”‚  â€¢ Content-addressable artifact mapping            â”‚ â”‚
â”‚ â”‚  â€¢ Session-scoped authentication                   â”‚ â”‚
â”‚ â”‚  â€¢ S3 backend with local cache tiers               â”‚ â”‚
â”‚ â”‚  â€¢ Automatic protocol detection & routing          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Micelio S3    â”‚
                  â”‚ (Global Cache)  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Zero-Configuration Activation

**Shell integration pattern (from Fabrik):**
```bash
# One-time setup
echo 'eval "$(hif activate zsh)"' >> ~/.zshrc

# Automatic activation on directory change
cd ~/my-project
# â†’ hif detects session context
# â†’ starts daemon with session-scoped identity  
# â†’ exports build tool environment variables
# â†’ all build commands transparently use cache
```

#### Protocol Translation Examples

**Bazel Remote Cache Protocol:**
```bash
# Daemon exports standard Bazel env vars
export BAZELRC=$HOME/.local/state/hif/sessions/abc123/bazelrc

# Auto-generated bazelrc content:
# build --remote_cache=grpc://localhost:8080
# build --remote_upload_local_results=true

# Bazel commands work unchanged
bazel build //...
# â†’ Talks to hif daemon via gRPC
# â†’ hif translates to S3 content-addressed storage
# â†’ Transparent caching across all agents
```

**Gradle Build Cache:**
```bash
# Daemon exports Gradle-specific URL
export GRADLE_BUILD_CACHE_URL=http://localhost:8080/gradle-cache/

# Gradle automatically uses remote cache
./gradlew build
# â†’ Gradle sends HTTP requests to hif daemon
# â†’ hif maps to S3 artifacts with session context
# â†’ Perfect cache sharing without configuration
```

**Docker Registry Protocol:**
```bash
# Daemon exposes Docker registry API
export DOCKER_REGISTRY=localhost:8080

# Docker commands work transparently  
docker build -t myapp .
# â†’ Docker pushes layers to hif daemon
# â†’ hif stores layers in S3 content-addressed
# â†’ Other agents get instant layer cache hits
```

#### Session-Scoped Daemon Management

**Per-session daemon isolation:**
```
hif session start "add-payments"
â”œâ”€â”€ Computes session hash: sha256:abc123...
â”œâ”€â”€ Spawns daemon: ~/.local/state/hif/sessions/abc123/
â”‚   â”œâ”€â”€ daemon.pid
â”‚   â”œâ”€â”€ ports.json â†’ {"http": 54321, "grpc": 54322}
â”‚   â”œâ”€â”€ session_identity â†’ time-bound S3 credentials
â”‚   â””â”€â”€ bazelrc â†’ auto-generated build tool configs
â”œâ”€â”€ Session ends â†’ daemon auto-terminates
â””â”€â”€ Credentials expire â†’ no lingering access
```

#### Multi-Toolchain Content Addressing

**Universal artifact mapping:**
```
Source changes hash: sha256:def456...
Build artifacts stored as:
â”œâ”€â”€ s3://forge/artifacts/bazel/def456/binary
â”œâ”€â”€ s3://forge/artifacts/gradle/def456/jar  
â”œâ”€â”€ s3://forge/artifacts/docker/def456/layers/
â””â”€â”€ s3://forge/artifacts/custom/def456/outputs/

Cross-toolchain deduplication:
â”œâ”€â”€ Same source hash = shared base artifacts
â”œâ”€â”€ Different toolchains = different artifact paths
â””â”€â”€ hif daemon handles mapping automatically
```

#### Advanced Cache Hierarchy

**Multi-tier caching strategy (inspired by Fabrik's P2P discovery):**
```
Agent cache lookup order:
1. Local filesystem cache (instant)
2. Local network P2P cache (1-5ms) 
3. Regional S3 bucket (10-50ms)
4. Global S3 bucket (50-200ms)
5. Rebuild locally (fallback)

hif daemon coordinates all tiers transparently
```

#### Build System Integration Matrix

| Build System | Protocol | Configuration | hif Integration |
|--------------|----------|---------------|-----------------|
| **Bazel** | gRPC Remote Cache | `BAZELRC` env var | Zero-config via auto-generated bazelrc |
| **Gradle** | HTTP Build Cache | `GRADLE_BUILD_CACHE_URL` | Zero-config via env var export |
| **Buck2** | gRPC Remote Cache | Command flags | Via shell alias or wrapper |
| **Nx** | HTTP Cache API | `NX_SELF_HOSTED_REMOTE_CACHE_SERVER` | Zero-config via env var |
| **TurboRepo** | HTTP API | `TURBO_API`, `TURBO_TOKEN` | Auto-generated token + URL |
| **Docker** | Registry Protocol | `DOCKER_REGISTRY` | Daemon exposes registry API |
| **sccache** | HTTP/S3 Protocol | `SCCACHE_ENDPOINT` | Compiler cache integration |
| **Custom** | HTTP REST | `CACHE_URL` | Generic HTTP cache interface |

#### Agent Workflow Integration

**Seamless integration with hif sessions:**
```
Session: "Optimize API performance"
â”œâ”€â”€ Goal: Reduce response time by 50ms
â”œâ”€â”€ Conversation: [agent reasoning about approach]
â”œâ”€â”€ Build Context:
â”‚   â”œâ”€â”€ Cache hits: 95% (Bazel remote cache)
â”‚   â”œâ”€â”€ Build time: 0.8s (mostly cached)
â”‚   â”œâ”€â”€ Test time: 2.1s (integration tests)
â”‚   â””â”€â”€ Total validation: 2.9s
â”œâ”€â”€ Decisions:
â”‚   â”œâ”€â”€ "Database connection pooling approach"
â”‚   â”œâ”€â”€ "All tests pass in <3s - confident change"
â”‚   â””â”€â”€ "Performance improvement verified"
â””â”€â”€ Land: Session includes build performance metrics
```

#### Implementation Benefits

**For Agents:**
- **Instant feedback**: 95%+ cache hit rates mean sub-second validation
- **Zero configuration**: All build tools work without modification
- **Consistent environments**: Nix + cached artifacts = identical results
- **Autonomous workflow**: No waiting for CI, no manual cache management

**For Organizations:**
- **Massive cost savings**: Shared cache eliminates redundant builds
- **Global consistency**: Same artifacts used everywhere
- **Security**: Session-scoped access, full audit trails
- **Scalability**: S3 handles unlimited storage, unlimited agents

**For Build Systems:**  
- **No modification required**: Existing build scripts work unchanged
- **Protocol compatibility**: Speaks native build system languages
- **Performance**: Local daemon eliminates network roundtrips for cache checks
- **Reliability**: Graceful degradation if cache unavailable

This daemon architecture provides the "narrow waist" that makes hif universally adoptable while enabling revolutionary agent workflows.

---

*Built by [Pedro PiÃ±era](https://github.com/pepicrft) and contributors. GPL-2.0 licensed.*